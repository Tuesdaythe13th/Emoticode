{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjl9khpJ8CoN"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/Google-Health/imaging-research/blob/master/ct-foundation/CT_Foundation_Demo.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/Google-Health/imaging-research/tree/master/ct-foundation\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2SSjj4fqzeOe"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnwIgD1bziUF"
      },
      "source": [
        "## CT Foundation API Demo\n",
        "The ipynb is a demonstration of using the\n",
        "[CT Foundation API](https://github.com/Google-Health/imaging-research/tree/master/ct-foundation)\n",
        "(this API computes embeddings for CT DICOMs).\n",
        "\n",
        "The contents include how to:\n",
        "\n",
        "-   Load the LIDC dataset from DICOMs stored in Google DICOM Store and labels stored in GCS\n",
        "-   Generate embeddings for the image files\n",
        "-   Train a small model using the embeddings\n",
        "\n",
        "**Note**: It can take some time to generate embeddings for thousands of images.\n",
        "For ease of use, by default, this colab uses precomputed embeddings. You can\n",
        "also calculate them from scratch again by updating the relevant param in the\n",
        "\"Global params\" section.\n",
        "\n",
        "### This notebook is for API demonstration purposes only\n",
        "\n",
        "**Note: This notebook is for API demonstration purposes only.**\n",
        "\n",
        "It's important to use evaluation datasets\n",
        "that reflect the expected distribution of images and patients you wish to use any downstream models on.\n",
        "\n",
        "This means that the best way to determine if this API is right for you is to try it with data that would be used for the downstream task you're interested in.\n",
        "\n",
        "**Note**: If you want to jump to training a model with embeddings, you can\n",
        "scroll down to [Train a model with the embeddings from NLST](#train-nlst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awoN_r8jOj0r"
      },
      "source": [
        "# Data Attribution\n",
        "\n",
        "This notebook makes use of two public datasets provided by the Cancer Imaging Archive which is managed by the United States  National Cancer Institute\n",
        "\n",
        "###  NLST Radiology CT Images CC BY 4.0\n",
        "[https://www.cancerimagingarchive.net/collection/nlst/](https://www.cancerimagingarchive.net/collection/nlst/)\n",
        "\n",
        "#### NLST Data Citation\n",
        " National Lung Screening Trial Research Team. (2013). Data from the National Lung Screening Trial (NLST) [Data set]. The Cancer Imaging Archive. https://doi.org/10.7937/TCIA.HMQ8-J677\n",
        "### LIDC-IDRI Data Access CC BY 3.0\n",
        "https://www.cancerimagingarchive.net/collection/lidc-idri/\n",
        "\n",
        "#### LIDC-IDRI Data Citation\n",
        "\n",
        "Armato III, S. G., McLennan, G., Bidaut, L., McNitt-Gray, M. F., Meyer, C. R., Reeves, A. P., Zhao, B., Aberle, D. R., Henschke, C. I., Hoffman, E. A., Kazerooni, E. A., MacMahon, H., Van Beek, E. J. R., Yankelevitz, D., Biancardi, A. M., Bland, P. H., Brown, M. S., Engelmann, R. M., Laderach, G. E., Max, D., Pais, R. C. , Qing, D. P. Y. , Roberts, R. Y., Smith, A. R., Starkey, A., Batra, P., Caligiuri, P., Farooqi, A., Gladish, G. W., Jude, C. M., Munden, R. F., Petkovska, I., Quint, L. E., Schwartz, L. H., Sundaram, B., Dodd, L. E., Fenimore, C., Gur, D., Petrick, N., Freymann, J., Kirby, J., Hughes, B., Casteele, A. V., Gupte, S., Sallam, M., Heath, M. D., Kuhn, M. H., Dharaiya, E., Burns, R., Fryd, D. S., Salganicoff, M., Anand, V., Shreter, U., Vastagh, S., Croft, B. Y., Clarke, L. P. (2015). Data From LIDC-IDRI [Data set]. The Cancer Imaging Archive. https://doi.org/10.7937/K9/TCIA.2015.LO9QL9SX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTpU6d_WPXx8"
      },
      "source": [
        "# Installation & Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfnWmzMCOwzG",
        "outputId": "4ead5700-3106-4d99-d369-76f53540d0cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (2.27.0)\n",
            "Collecting requests-toolbelt\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting dicomweb-client[gcp]\n",
            "  Downloading dicomweb_client-0.59.3-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from dicomweb-client[gcp]) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.10/dist-packages (from dicomweb-client[gcp]) (2.32.3)\n",
            "Collecting retrying>=1.3.3 (from dicomweb-client[gcp])\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: Pillow>=8.3 in /usr/local/lib/python3.10/dist-packages (from dicomweb-client[gcp]) (10.4.0)\n",
            "Collecting pydicom>=2.2 (from dicomweb-client[gcp])\n",
            "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18->dicomweb-client[gcp]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18->dicomweb-client[gcp]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18->dicomweb-client[gcp]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18->dicomweb-client[gcp]) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying>=1.3.3->dicomweb-client[gcp]) (1.16.0)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading dicomweb_client-0.59.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: retrying, pydicom, requests-toolbelt, dicomweb-client\n",
            "Successfully installed dicomweb-client-0.59.3 pydicom-3.0.1 requests-toolbelt-1.0.0 retrying-1.3.4\n",
            "Collecting tf-models-official==2.14.0\n",
            "  Downloading tf_models_official-2.14.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (3.0.11)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (10.4.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (2.137.0)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (4.2.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (1.6.17)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (1.26.4)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (4.1.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (4.10.0.84)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (9.0.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (2.0.8)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (6.0.2)\n",
            "Collecting sacrebleu (from tf-models-official==2.14.0)\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (1.13.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (0.2.0)\n",
            "Collecting seqeval (from tf-models-official==2.14.0)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (4.9.6)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (0.16.1)\n",
            "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official==2.14.0)\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Collecting tensorflow-text~=2.14.0 (from tf-models-official==2.14.0)\n",
            "  Downloading tensorflow_text-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow~=2.14.0 (from tf-models-official==2.14.0)\n",
            "  Downloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.14.0) (1.1.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.14.0) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.14.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.14.0) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.14.0) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.14.0) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.14.0) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.14.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.14.0) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.14.0) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.14.0) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.14.0) (6.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->tf-models-official==2.14.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->tf-models-official==2.14.0) (2024.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (18.1.1)\n",
            "Collecting ml-dtypes==0.2.0 (from tensorflow~=2.14.0->tf-models-official==2.14.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (75.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow~=2.14.0->tf-models-official==2.14.0)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->tf-models-official==2.14.0) (1.64.1)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow~=2.14.0->tf-models-official==2.14.0)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow~=2.14.0->tf-models-official==2.14.0)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.15,>=2.14.0 (from tensorflow~=2.14.0->tf-models-official==2.14.0)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.6.0->tf-models-official==2.14.0) (2.17.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.14.0) (0.1.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.14.0) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.14.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.14.0) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.14.0) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.14.0) (3.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.14.0) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.14.0) (0.4.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.14.0) (4.9)\n",
            "Collecting portalocker (from sacrebleu->tf-models-official==2.14.0)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official==2.14.0) (2024.9.11)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official==2.14.0) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->tf-models-official==2.14.0)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official==2.14.0) (4.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official==2.14.0) (1.5.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.14.0) (8.1.7)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.14.0) (2.3)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.14.0) (16.1.0)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.14.0) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.14.0) (1.16.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.14.0) (0.10.2)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.14.0) (0.5.1)\n",
            "Requirement already satisfied: etils>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.14.0) (1.10.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.14.0->tf-models-official==2.14.0) (0.44.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.14.0) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.14.0) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.14.0) (3.20.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.14.0) (1.65.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.14.0) (1.24.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official==2.14.0) (5.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle>=1.3.9->tf-models-official==2.14.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle>=1.3.9->tf-models-official==2.14.0) (3.10)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.14.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.14.0) (3.5.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official==2.14.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official==2.14.0) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official==2.14.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official==2.14.0) (3.0.4)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub>=0.6.0->tf-models-official==2.14.0)\n",
            "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading tf_keras-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading tf_keras-2.15.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official==2.14.0) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.14.0) (1.3)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets->tf-models-official==2.14.0) (0.16)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official==2.14.0) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official==2.14.0) (3.0.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official==2.14.0) (3.2.2)\n",
            "Downloading tf_models_official-2.14.0-py2.py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_text-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=50cdcb2eb6668e398878003d1491aeb36a69bcd0d8bcebf76fd25b37aeb106c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: wrapt, tf-keras, tensorflow-model-optimization, tensorflow-estimator, portalocker, ml-dtypes, keras, colorama, sacrebleu, seqeval, google-auth-oauthlib, tensorboard, tensorflow, tensorflow-text, tf-models-official\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.17.0\n",
            "    Uninstalling tf_keras-2.17.0:\n",
            "      Successfully uninstalled tf_keras-2.17.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n"
          ]
        }
      ],
      "source": [
        "# Notebook specific dependencies\n",
        "# !pip install google_health.ct_dicom\n",
        "# TODO OPTIONAL: Create pip installation for the code below - https://github.com/Google-Health/google-health/tree/master/ct_dicom\n",
        "\n",
        "!pip install absl-py dicomweb-client[gcp] google-auth requests-toolbelt\n",
        "!pip install tf-models-official==2.14.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYMxVPMPzpR7"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import http\n",
        "import matplotlib\n",
        "import pydicom\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from typing import Iterable, Optional\n",
        "from google.colab import auth\n",
        "from google.oauth2 import credentials\n",
        "import pandas as pd\n",
        "import dicomweb_client.ext.gcp.uri as gcp_uri\n",
        "import dicomweb_client.uri as dicomweb_uri\n",
        "from google.colab import auth\n",
        "from google.oauth2 import credentials\n",
        "from google.auth import credentials as gcredentials\n",
        "from google.auth.transport import requests\n",
        "from requests_toolbelt.multipart import decoder\n",
        "from google.cloud import storage\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5_an_XYPdBA"
      },
      "source": [
        "**IMPORTANT**: If you are using Colab, you must restart the runtime after installing new packages.\n",
        "\n",
        "NOTE: There will be some ERROR messages due to the protobuf library - this is normal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7sY6uzinTGjg"
      },
      "outputs": [],
      "source": [
        "#@title Classes for testing data access and visualization\n",
        "\n",
        "\"\"\"Google Cloud Healthcare (CHC) DICOMweb utilities.\"\"\"\n",
        "\n",
        "# Well-known constants from https://www.dicomstandard.org/.\n",
        "_STUDY_INSTANCE_UID_TAG = '0020000D'\n",
        "_SERIES_INSTANCE_UID_TAG = '0020000E'\n",
        "_SOP_INSTANCE_UID_TAG = '00080018'\n",
        "\n",
        "_SERIES_INSTANCE_UID_SEARCH_SUFFIX = 'series'\n",
        "_STUDY_INSTANCE_UID_SEARCH_SUFFIX = 'studies'\n",
        "_SOP_INSTANCE_UID_SEARCH_SUFFIX = 'instances'\n",
        "\n",
        "_VALUE_KEY = 'Value'\n",
        "\n",
        "# Scope requirements from:\n",
        "# https://cloud.google.com/healthcare-api/docs/reference/rest/v1/projects.locations.datasets.dicomStores/searchForInstances#authorization-scopes\n",
        "_AUTHORIZATION_SCOPES = ['https://www.googleapis.com/auth/cloud-healthcare']\n",
        "\n",
        "# Search result limits for the CHC DICOMweb API:\n",
        "# https://cloud.google.com/healthcare-api/docs/dicom#search_parameters\n",
        "_MAX_LIMIT_STUDY = 5000\n",
        "_MAX_LIMIT_SERIES = 5000\n",
        "_MAX_LIMIT_SOP = 50000\n",
        "_MAX_OFFSET = 1000000\n",
        "\n",
        "_MAX_REFRESH_ATTEMPTS = 10\n",
        "_REQUEST_TIMEOUT_SECONDS = 600\n",
        "\n",
        "\n",
        "def create_authorized_session(\n",
        "    credentials: gcredentials.Credentials,\n",
        ") -> requests.AuthorizedSession:\n",
        "  \"\"\"Creates a Session authorized for Cloud Healthcare API interactions.\n",
        "\n",
        "  Args:\n",
        "    credentials: Google Auth credentials. For further details, see\n",
        "      https://googleapis.dev/python/google-auth/latest/index.html.\n",
        "\n",
        "  Returns:\n",
        "    Credentials object with the requisite API scope.\n",
        "  \"\"\"\n",
        "  authorization_scopes = _AUTHORIZATION_SCOPES\n",
        "  scoped_credentials = gcredentials.with_scopes_if_required(\n",
        "      credentials, authorization_scopes\n",
        "  )\n",
        "  return requests.AuthorizedSession(\n",
        "      scoped_credentials, max_refresh_attempts=_MAX_REFRESH_ATTEMPTS\n",
        "  )\n",
        "\n",
        "\n",
        "def download_multipart_dicom_series(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    dataset_id: str,\n",
        "    dicom_store_id: str,\n",
        "    session: requests.AuthorizedSession,\n",
        "    study_instance_uid: str,\n",
        "    series_instance_uid: str,\n",
        ") -> Iterable[bytes]:\n",
        "  \"\"\"Downloads all SOP Instances (DICOMs) within a Series Instance UID.\n",
        "\n",
        "  The request accepts a multipart MIME response from the CHC DICOMweb API to\n",
        "  reduce the:\n",
        "  - Latency associated with making one API call per Instance.\n",
        "  - API quota usage while downloading all Instances within a Series.\n",
        "\n",
        "  Args:\n",
        "    project_id: The GCP Project containing the DICOM Store to query.\n",
        "    location: The regional location associated with the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/regions).\n",
        "    dataset_id: The Dataset containing the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/projects-datasets-data-stores)\n",
        "    dicom_store_id: The DICOM Store to query.\n",
        "    session: An Google Auth session authorized to use the CHC DICOMweb API.\n",
        "    study_instance_uid: The Study Instance UID containing the Series Instance\n",
        "      UID to download.\n",
        "    series_instance_uid: The Series Instance UID containing the SOP Instances\n",
        "      (DICOMs) to download.\n",
        "\n",
        "  Yields:\n",
        "    DICOM bytes associated with each Instance contained within the input Series\n",
        "    Instance UID.\n",
        "  \"\"\"\n",
        "  dicomweb_path = str(\n",
        "      dicomweb_uri.URI(\n",
        "          str(\n",
        "              gcp_uri.GoogleCloudHealthcareURL(\n",
        "                  project_id, location, dataset_id, dicom_store_id\n",
        "              )\n",
        "          ),\n",
        "          study_instance_uid,\n",
        "          series_instance_uid,\n",
        "      )\n",
        "  )\n",
        "\n",
        "  headers = {\n",
        "      'Accept': (\n",
        "          'multipart/related; transfer-syntax=1.2.840.10008.1.2.1;'\n",
        "          ' type=\"application/dicom\"'\n",
        "      )\n",
        "  }\n",
        "  response = session.get(\n",
        "      dicomweb_path, headers=headers, timeout=_REQUEST_TIMEOUT_SECONDS\n",
        "  )\n",
        "  response.raise_for_status()\n",
        "\n",
        "  for part in decoder.MultipartDecoder.from_response(response).parts:\n",
        "    yield part.content\n",
        "\n",
        "\n",
        "def search_study_instance_uids(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    dataset_id: str,\n",
        "    dicom_store_id: str,\n",
        "    session: requests.AuthorizedSession,\n",
        "    limit: int = 100,\n",
        ") -> Iterable[str]:\n",
        "  \"\"\"Recovers all Study Instance UIDs from a CHC DICOM Store.\n",
        "\n",
        "  Args:\n",
        "    project_id: The GCP Project containing the DICOM Store to query.\n",
        "    location: The regional location associated with the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/regions).\n",
        "    dataset_id: The Dataset containing the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/projects-datasets-data-stores)\n",
        "    dicom_store_id: The DICOM Store to query.\n",
        "    session: An Google Auth session authorized to use the CHC DICOMweb API.\n",
        "    limit: The number of Study Instance UIDs in the DICOM Store could be large.\n",
        "      The UIDs are recovered in a paginated fashion, where each page of results\n",
        "      (one page per query) includes at most `limit` values. The higher this\n",
        "      value, the fewer the total number of requests, but each response would be\n",
        "      larger. Depending on your network connection, set this value in the range\n",
        "      1 through 5000 (both inclusive). This parameter impacts the speed and\n",
        "      network bandwidth utilization, but not the values returned by the method.\n",
        "\n",
        "  Yields:\n",
        "    Study Instance UIDs from the DICOM Store.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If `limit` exceeds the max value of 5000 allowed by the CHC\n",
        "      DICOMweb API (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/projects-datasets-data-stores)\n",
        "  \"\"\"\n",
        "  if limit > _MAX_LIMIT_STUDY:\n",
        "    raise ValueError(\n",
        "        f'Request limit {limit} exceeds the CHC Search query request limit of'\n",
        "        f' {_MAX_LIMIT_STUDY} for Study Instances.'\n",
        "    )\n",
        "  yield from _search_dicom_data(\n",
        "      project_id,\n",
        "      location,\n",
        "      dataset_id,\n",
        "      dicom_store_id,\n",
        "      _STUDY_INSTANCE_UID_SEARCH_SUFFIX,\n",
        "      _STUDY_INSTANCE_UID_TAG,\n",
        "      session,\n",
        "      limit,\n",
        "  )\n",
        "\n",
        "\n",
        "def search_series_instance_uids(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    dataset_id: str,\n",
        "    dicom_store_id: str,\n",
        "    session: requests.AuthorizedSession,\n",
        "    study_instance_uid: Optional[str] = None,\n",
        "    limit: int = 100,\n",
        ") -> Iterable[str]:\n",
        "  \"\"\"Recovers all Series Instance UIDs from a CHC DICOM Store.\n",
        "\n",
        "  The scope may be restricted to all Series within a fixed Study Instance\n",
        "  UIDs (see `study_instance_uid` below).\n",
        "\n",
        "  Args:\n",
        "    project_id: The GCP Project containing the DICOM Store to query.\n",
        "    location: The regional location associated with the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/regions).\n",
        "    dataset_id: The Dataset containing the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/projects-datasets-data-stores)\n",
        "    dicom_store_id: The DICOM Store to query.\n",
        "    session: An Google Auth session authorized to use the CHC DICOMweb API.\n",
        "    study_instance_uid: If provided, restricts the returned Series Instance UIDs\n",
        "      to within this Study Instance UID.\n",
        "    limit: The number of Study Instance UIDs in the DICOM Store could be large.\n",
        "      The UIDs are recovered in a paginated fashion, where each page (query)\n",
        "      includes at most `limit` values. The higher this value, the fewer the\n",
        "      total number of requests, but each response would be larger. Depending on\n",
        "      your network connection, set this value in the range 1 through 5000 (both\n",
        "      inclusive).\n",
        "\n",
        "  Yields:\n",
        "    Series Instance UIDs from the DICOM Store (optionally within the scope of\n",
        "    the input `study_instance_uid`, if provided).\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If `limit` exceeds the max value of 5000 allowed by the CHC\n",
        "      DICOMweb API (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/projects-datasets-data-stores)\n",
        "  \"\"\"\n",
        "  if limit > _MAX_LIMIT_SERIES:\n",
        "    raise ValueError(\n",
        "        f'Request limit {limit} exceeds the CHC Search query request limit of'\n",
        "        f' {_MAX_LIMIT_SERIES} for Series Instances.'\n",
        "    )\n",
        "  search_suffix = (\n",
        "      _SERIES_INSTANCE_UID_SEARCH_SUFFIX\n",
        "      if study_instance_uid is None\n",
        "      else f'studies/{study_instance_uid}/series'\n",
        "  )\n",
        "  yield from _search_dicom_data(\n",
        "      project_id,\n",
        "      location,\n",
        "      dataset_id,\n",
        "      dicom_store_id,\n",
        "      search_suffix,\n",
        "      _SERIES_INSTANCE_UID_TAG,\n",
        "      session,\n",
        "      limit,\n",
        "  )\n",
        "\n",
        "\n",
        "def search_sop_instance_uids(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    dataset_id: str,\n",
        "    dicom_store_id: str,\n",
        "    session: requests.AuthorizedSession,\n",
        "    study_instance_uid: Optional[str] = None,\n",
        "    series_instance_uid: Optional[str] = None,\n",
        "    limit: int = 1000,\n",
        ") -> Iterable[str]:\n",
        "  \"\"\"Recovers all SOP Instance UIDs from a CHC DICOM Store.\n",
        "\n",
        "  The scope may be restricted to all Series within a fixed:\n",
        "  - Study Instance UID.\n",
        "  - Study and Series Instance UID pair.\n",
        "\n",
        "  (see `study_instance_uid` and `series_instance_uid` below).\n",
        "\n",
        "  Args:\n",
        "    project_id: The GCP Project containing the DICOM Store to query.\n",
        "    location: The regional location associated with the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/regions).\n",
        "    dataset_id: The Dataset containing the DICOM Store (c.f.\n",
        "      https://cloud.google.com/healthcare-api/docs/projects-datasets-data-stores)\n",
        "    dicom_store_id: The DICOM Store to query.\n",
        "    session: An Google Auth session authorized to use the CHC DICOMweb API.\n",
        "    study_instance_uid: If provided, restricts the returned SOP Instance UIDs to\n",
        "      within this Study Instance UID.\n",
        "    series_instance_uid: If provided, restricts the returned Series Instance\n",
        "      UIDs to within this Series Instance UID. The associated Study Instance UID\n",
        "      must also be specified.\n",
        "    limit: The number of SOP Instance UIDs in the DICOM Store could be large.\n",
        "      The UIDs are recovered in a paginated fashion, where each page (query)\n",
        "      includes at most `limit` values. The higher this value, the fewer the\n",
        "      total number of requests, but each response would be larger. Depending on\n",
        "      your network connection, set this value in the range 1 through 5000 (both\n",
        "      inclusive).\n",
        "\n",
        "  Yields:\n",
        "    SOP Instance UIDs from the DICOM Store (optionally within the scope of\n",
        "    the input `study_instance_uid` and/or `series_instance_uid`, if provided).\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If\n",
        "      - `limit` exceeds the max value of 50000 allowed by the CHC DICOMweb API\n",
        "        (c.f.\n",
        "        https://cloud.google.com/healthcare-api/docs/projects-datasets-data-stores).\n",
        "      - `series_instance_uid` is specified by `study_instance_uid` is not.\n",
        "  \"\"\"\n",
        "  if limit > _MAX_LIMIT_SOP:\n",
        "    raise ValueError(\n",
        "        f'Request limit {limit} exceeds the CHC Search query request limit of'\n",
        "        f' {_MAX_LIMIT_SOP} for SOP Instances.'\n",
        "    )\n",
        "\n",
        "  if study_instance_uid is None and series_instance_uid is None:\n",
        "    search_suffix = _SOP_INSTANCE_UID_SEARCH_SUFFIX\n",
        "  elif series_instance_uid is None:\n",
        "    search_suffix = f'studies/{study_instance_uid}/instances'\n",
        "  elif study_instance_uid is None:\n",
        "    raise ValueError(\n",
        "        'Study Instance UID must be provided if Series Instance UID is'\n",
        "        ' specified.'\n",
        "    )\n",
        "  else:\n",
        "    search_suffix = (\n",
        "        f'studies/{study_instance_uid}/series/{series_instance_uid}/instances'\n",
        "    )\n",
        "  yield from _search_dicom_data(\n",
        "      project_id,\n",
        "      location,\n",
        "      dataset_id,\n",
        "      dicom_store_id,\n",
        "      search_suffix,\n",
        "      _SOP_INSTANCE_UID_TAG,\n",
        "      session,\n",
        "      limit,\n",
        "  )\n",
        "\n",
        "\n",
        "def _search_dicom_data(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    dataset_id: str,\n",
        "    dicom_store_id: str,\n",
        "    query_suffix: str,\n",
        "    dicom_tag: str,\n",
        "    session: requests.AuthorizedSession,\n",
        "    limit: int,\n",
        ") -> Iterable[str]:\n",
        "  \"\"\"Generates DICOM UIDs from a CHC DICOM Store.\"\"\"\n",
        "  assert limit > 0\n",
        "\n",
        "  uri = gcp_uri.GoogleCloudHealthcareURL(\n",
        "      project_id, location, dataset_id, dicom_store_id\n",
        "  )\n",
        "  base_dicomweb_query_path = f'{uri}/{query_suffix}?includefield={dicom_tag}'\n",
        "  headers = {'Content-Type': 'application/dicom+json; charset=utf-8'}\n",
        "\n",
        "  # The CHC offset limit puts an upper bound on the Instance count, which is\n",
        "  # also used to limit the number of iterations.\n",
        "  for offset in range(0, _MAX_OFFSET, limit):\n",
        "    dicomweb_query_path = (\n",
        "        f'{base_dicomweb_query_path}&offset={offset}&limit={limit}'\n",
        "    )\n",
        "\n",
        "    response = session.get(\n",
        "        dicomweb_query_path, headers=headers, timeout=_REQUEST_TIMEOUT_SECONDS\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    # CHC DICOMweb API does not set a Warning response header on the last\n",
        "    # available page:\n",
        "    # https://cloud.google.com/healthcare-api/docs/dicom#search_parameters\n",
        "    if response.status_code == http.HTTPStatus.NO_CONTENT:\n",
        "      return\n",
        "\n",
        "    for instance in response.json():\n",
        "      assert dicom_tag in instance\n",
        "      assert _VALUE_KEY in instance[dicom_tag]\n",
        "\n",
        "      for value in instance[dicom_tag][_VALUE_KEY]:\n",
        "        yield value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb82fhNmYF1P"
      },
      "outputs": [],
      "source": [
        "# @title Authenticate\n",
        "# Authenticate user for access. There will be a popup asking you to sign in with your user and approve access.\n",
        "auth.authenticate_user()\n",
        "TOKEN_ = !gcloud beta auth application-default print-access-token\n",
        "TOKEN = TOKEN_[0]\n",
        "\n",
        "# This is your token for accessing the API and CT Volumes.\n",
        "# It's good for 1 hour until you need a new one.\n",
        "TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfmFZPCFM2G2"
      },
      "outputs": [],
      "source": [
        "# @title Set DICOM store parameters\n",
        "project_id='hai-cd3-foundations' # @param {type:\"string\"}\n",
        "location='us-central1'  # @param {type:\"string\"}\n",
        "dataset_id='ct3d'  # @param {type:\"string\"}\n",
        "dicom_store_id='lidc-idri' # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JQRxrXiUUgv"
      },
      "source": [
        "<a name=\"train-nlst\"></a>\n",
        "# Train a model with the embeddings from NLST\n",
        "\n",
        "Here we have a full set of embeddings from the NLST dataset that you can download and train a cancer detection model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkB5R0sx7C7Z"
      },
      "source": [
        "## Collect the stored NPZ data from the cloud bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzI0A92hH5VZ"
      },
      "outputs": [],
      "source": [
        "# GCS bucket with data to read:\n",
        "gcs_storage_client = storage.Client(project_id) # our bucket is in the same Google Cloud Project as the DicomStore\n",
        "gcs_bucket_name = 'hai-cd3-foundations-ct3d-vault-entry'# @param {type:\"string\"}\n",
        "gcs_bucket = gcs_storage_client.bucket(gcs_bucket_name)\n",
        "tune_path = 'nlst/nlst_tune_with_labels.npz' # @param {type:\"string\"}\n",
        "train_path = 'nlst/nlst_train_with_labels.npz' # @param {type:\"string\"}\n",
        "\n",
        "def read_embeddings(path):\n",
        "  blob = gcs_bucket.blob(path)\n",
        "  with blob.open('rb') as f:\n",
        "    data = np.load(f,allow_pickle=True)\n",
        "    key = data.files[0]\n",
        "    return pd.DataFrame.from_dict(data[key].item(), orient='index')\n",
        "\n",
        "df_tune = read_embeddings(tune_path)\n",
        "df_train = read_embeddings(train_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o15b23RR7WYz"
      },
      "source": [
        "## Train and Evaluate a model using precomputed embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1cEUPaH90DV"
      },
      "outputs": [],
      "source": [
        "# choose whether you want predict whether the screen leads to a positive lung cancer diagnosis within 1 or 2 years.\n",
        "model_head = 'cancer_in_2' # @param [\"cancer_in_1\",\"cancer_in_2\"]\n",
        "# Get NumPy arrays from DataFrames\n",
        "train_embeddings = df_train.embedding\n",
        "train_labels = df_train[model_head].values\n",
        "tune_embeddings = df_tune.embedding\n",
        "tune_labels = df_tune[model_head].values\n",
        "\n",
        "# Convert the NumPy arrays to ragged tensors\n",
        "train_embeddings = tf.constant(list(train_embeddings))\n",
        "tune_embeddings = tf.convert_to_tensor(list(tune_embeddings))\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_embeddings, train_labels))\n",
        "eval_ds = tf.data.Dataset.from_tensor_slices((tune_embeddings, tune_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnAmHU5mA88c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_models as tfm\n",
        "\n",
        "\n",
        "def create_model(heads,\n",
        "                 token_num=1,\n",
        "                 embeddings_size=1408,\n",
        "                 learning_rate=0.07,\n",
        "                 end_lr_factor=1.0,\n",
        "                 dropout=0.5,\n",
        "                 loss_weights=None,\n",
        "                 hidden_layer_sizes=[128, 32],\n",
        "                 weight_decay=0.0001,\n",
        "                 seed=None) -> tf.keras.Model:\n",
        "  \"\"\"\n",
        "  Creates linear probe or multilayer perceptron using LARS.\n",
        "\n",
        "  \"\"\"\n",
        "  inputs = tf.keras.Input(shape=(token_num * embeddings_size,))\n",
        "  inputs_reshape = tf.keras.layers.Reshape((token_num, embeddings_size))(inputs)\n",
        "  inputs_pooled = tf.keras.layers.GlobalAveragePooling1D(data_format='channels_last')(inputs_reshape)\n",
        "  hidden = inputs_pooled\n",
        "  # If no hidden_layer_sizes are provided, model will be a linear probe.\n",
        "  for size in hidden_layer_sizes:\n",
        "    hidden = tf.keras.layers.Dense(\n",
        "        size,\n",
        "        activation='relu',\n",
        "        kernel_initializer=tf.keras.initializers.HeUniform(seed=seed),\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(l2=weight_decay),\n",
        "        bias_regularizer=tf.keras.regularizers.l2(l2=weight_decay))(\n",
        "            hidden)\n",
        "    hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
        "    hidden = tf.keras.layers.Dropout(dropout, seed=seed)(hidden)\n",
        "  output = tf.keras.layers.Dense(\n",
        "      units=len(heads),\n",
        "      activation='sigmoid',\n",
        "      kernel_initializer=tf.keras.initializers.HeUniform(seed=seed))(\n",
        "          hidden)\n",
        "\n",
        "  outputs = {}\n",
        "  for i, head in enumerate(heads):\n",
        "    outputs[head] = tf.keras.layers.Lambda(\n",
        "        lambda x: x[..., i:i + 1], name=head.lower())(\n",
        "            output)\n",
        "\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  model.compile(\n",
        "      optimizer=tfm.optimization.lars.LARS(\n",
        "         learning_rate=.1),\n",
        "      loss=dict([(head, 'binary_focal_crossentropy') for head in heads]),\n",
        "      loss_weights=loss_weights or 1.0,\n",
        "      weighted_metrics=[\n",
        "        tf.keras.metrics.FalsePositives(),\n",
        "        tf.keras.metrics.FalseNegatives(),\n",
        "        tf.keras.metrics.TruePositives(),\n",
        "        tf.keras.metrics.TrueNegatives(),\n",
        "        tf.keras.metrics.AUC(),\n",
        "        tf.keras.metrics.AUC(curve='PR', name='auc_pr')])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZIUOHpWEyj5"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "DIAGNOSIS = 'cancer_in_2'\n",
        "model = create_model(\n",
        "    [DIAGNOSIS]\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    x=train_ds.batch(512).prefetch(tf.data.AUTOTUNE).cache(),\n",
        "    validation_data=eval_ds.batch(32).cache(),\n",
        "    epochs=35,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh24XhCaE15V"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_curve(x, y, auc, x_label=None, y_label=None, label=None):\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  plt.plot(x, y, label=f'{label} (AUC: %.3f)' % auc, color='black')\n",
        "  plt.legend(loc='lower right', fontsize=18)\n",
        "  plt.xlim([-0.01, 1.01])\n",
        "  plt.ylim([-0.01, 1.01])\n",
        "  if x_label:\n",
        "    plt.xlabel(x_label, fontsize=24)\n",
        "  if y_label:\n",
        "    plt.ylabel(y_label, fontsize=24)\n",
        "  plt.xticks(fontsize=12)\n",
        "  plt.yticks(fontsize=12)\n",
        "  plt.grid(visible=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-n_8DvClC9Z"
      },
      "outputs": [],
      "source": [
        "rows = []\n",
        "for embeddings, label in eval_ds.batch(1):\n",
        "  row = {\n",
        "      f'{DIAGNOSIS}_prediction': model(embeddings)[DIAGNOSIS].numpy().flatten()[0],\n",
        "      f'{DIAGNOSIS}_value': label.numpy().flatten()[0]\n",
        "  }\n",
        "  rows.append(row)\n",
        "eval_df = pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULHjSMx8lFyY"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "labels = eval_df[f'{DIAGNOSIS}_value'].values\n",
        "predictions = eval_df[f'{DIAGNOSIS}_prediction'].values\n",
        "false_positive_rate, true_positive_rate, thresholds = sklearn.metrics.roc_curve(\n",
        "    labels,\n",
        "    predictions,\n",
        "    drop_intermediate=False)\n",
        "auc = sklearn.metrics.roc_auc_score(labels, predictions)\n",
        "plot_curve(false_positive_rate, true_positive_rate, auc, x_label='False Positive Rate', y_label='True Positive Rate', label=DIAGNOSIS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo_McpwVQRCV"
      },
      "source": [
        "# Test access to LIDC DICOM store and the API\n",
        "\n",
        "Get a token that grants access to the DICOM store and use it to download a volume via the DICOMWEb API. Next, we can collect the embeddings from the\n",
        "selected embedding.\n",
        "\n",
        "**NOTE**: You can skip this section if you just want to train a model on NLST data using the embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szMANNgm4crh"
      },
      "source": [
        "## Download a CT volume to visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VOuJFzaXlWe"
      },
      "outputs": [],
      "source": [
        "# @title Create a Session via a token.\n",
        "creds = credentials.Credentials(TOKEN)\n",
        "session = create_authorized_session(creds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0qKyk_7ZLfK",
        "outputId": "005465b9-934a-4e3e-abab-39cea2179afa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Slices in downloaded volume: 219\n"
          ]
        }
      ],
      "source": [
        "# @title List the study and series instance UIDs and download a single volume.\n",
        "\n",
        "TOTAL_VOLUMES = 5  # Total number to list out.\n",
        "VOLUME_TO_SHOW = 1 # Which ones to render below\n",
        "\n",
        "study_uids = list(search_study_instance_uids(\n",
        "    project_id=project_id,\n",
        "    location=location,\n",
        "    dataset_id=dataset_id,\n",
        "    dicom_store_id=dicom_store_id,\n",
        "    session=session))\n",
        "\n",
        "\n",
        "corresponding_series_uids = []\n",
        "for study_number, a_study_uid in enumerate(study_uids):\n",
        "  a_series = list(search_series_instance_uids(project_id=project_id,\n",
        "    location=location,\n",
        "    dataset_id=dataset_id,\n",
        "    dicom_store_id=dicom_store_id,\n",
        "    session=session, study_instance_uid=a_study_uid))[0]\n",
        "  corresponding_series_uids.append(a_series)\n",
        "  if study_number == TOTAL_VOLUMES:\n",
        "    break\n",
        "\n",
        "\n",
        "volume_as_bytes = list(download_multipart_dicom_series(\n",
        "      project_id='hai-cd3-foundations',\n",
        "      location='us-central1',\n",
        "      dataset_id='ct3d',\n",
        "      dicom_store_id='lidc-idri',\n",
        "      session=session,\n",
        "      study_instance_uid=study_uids[VOLUME_TO_SHOW],\n",
        "      series_instance_uid=corresponding_series_uids[VOLUME_TO_SHOW],\n",
        "  ))\n",
        "\n",
        "print(f'Total Slices in downloaded volume: {len(volume_as_bytes)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJUE96Qh0ZQY"
      },
      "outputs": [],
      "source": [
        "#@title Render a single slice\n",
        "SLICE_TO_RENDER = 30\n",
        "example_dicom = pydicom.dcmread(io.BytesIO(volume_as_bytes[SLICE_TO_RENDER]))\n",
        "\n",
        "arr_unsigned = example_dicom.pixel_array.copy()\n",
        "arr_unsigned = arr_unsigned.astype(np.float32)\n",
        "arr_unsigned[arr_unsigned <0] = 0\n",
        "arr_unsigned[arr_unsigned >1000] = 1000\n",
        "arr_unsigned = (arr_unsigned / 1000) * 255\n",
        "arr_unsigned = arr_unsigned.astype(np.uint8)\n",
        "Image.fromarray(arr_unsigned, mode='L')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPOseIHV4lAF"
      },
      "source": [
        "## Call the API to compute embeddings for the selected volume."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzal615K4rDv"
      },
      "outputs": [],
      "source": [
        "#@title Define method to call API and create an LIDC URL.\n",
        "\n",
        "from google.api_core.client_options import ClientOptions\n",
        "from google.api_core.retry import Retry\n",
        "from google.cloud import aiplatform\n",
        "from google.api_core import exceptions\n",
        "_API_ENDPOINT = 'us-west1-aiplatform.googleapis.com'\n",
        "\n",
        "\n",
        "def create_lidc_series_url(study_instance_uid, series_instance_uid):\n",
        "  return ('https://healthcare.googleapis.com/v1/projects/hai-cd3-foundations/'\n",
        "          'locations/us-central1/datasets/ct3d/dicomStores/lidc-idri/dicomWeb/'\n",
        "          f'studies/{study_instance_uid}/series/{series_instance_uid}')\n",
        "\n",
        "\n",
        "_RETRIABLE_TYPES = (\n",
        "    exceptions.TooManyRequests,  # HTTP 429\n",
        "    exceptions.InternalServerError,  # HTTP 500\n",
        "    exceptions.BadGateway,  # HTTP 502\n",
        "    exceptions.ServiceUnavailable,  # HTTP 503\n",
        "    exceptions.DeadlineExceeded,  # HTTP 504\n",
        ")\n",
        "\n",
        "\n",
        "def _is_retryable(exc):\n",
        "  return isinstance(exc, _RETRIABLE_TYPES)\n",
        "\n",
        "\n",
        "retry_policy = Retry(predicate=_is_retryable)\n",
        "\n",
        "\n",
        "def get_ct_embeddings(mytoken: str, dicom_urls: List[str]) -> List[Any]:\n",
        "  \"\"\"Calls the API to collect the embeddings from a given volume.\n",
        "\n",
        "  Args:\n",
        "    mytoken: The token to access the DICOM volume and API.\n",
        "    dicom_urls: The list of Series-level DICOM URL to the CT volumes.\n",
        "\n",
        "  Returns:\n",
        "    The list of embeddings or errors generated by the service. Differences in\n",
        "    Vertex end-point configurations may change the return type. The caller is\n",
        "    responsible for interpreting this value and extracting the requisite\n",
        "    data.\n",
        "\n",
        "  \"\"\"\n",
        "  api_client = aiplatform.gapic.PredictionServiceClient(\n",
        "      client_options=ClientOptions(api_endpoint=_API_ENDPOINT)\n",
        "  )\n",
        "  endpoint = api_client.endpoint_path(\n",
        "      project='hai-cd3-foundations', location='us-central1', endpoint=300\n",
        "  )\n",
        "\n",
        "  # Create a single instance to send - you can send up to 5.\n",
        "  instances = []\n",
        "  for dicom_url in dicom_urls:\n",
        "    instances.append({\"dicom_path\": dicom_url, \"bearer_token\": mytoken})\n",
        "\n",
        "  response = api_client.predict(\n",
        "      endpoint=endpoint, instances=[instance], retry=retry_policy, timeout=600\n",
        "  )\n",
        "  assert len(response.predictions) == len(dicom_urls)\n",
        "  assert len(response.predictions[0]) == 3\n",
        "  # You can get the model version for this prediction\n",
        "  # response.predictions[0]['model_version']\n",
        "\n",
        "  # Check for no errors\n",
        "  responses = []\n",
        "  for a_prediction in response.predictions:\n",
        "    if a_prediction['error_response'] is not None:\n",
        "      responses.append(a_prediction['error_response'])\n",
        "    else:\n",
        "      embeddings = np.array(a_prediction['embedding_result']['embedding'],\n",
        "                            dtype=np.float32)\n",
        "      assert embeddings.shape == (1408,), 'Unexpected embeddings shape.'\n",
        "      responses.append(embeddings)\n",
        "\n",
        "  return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_YHl2E30nHo"
      },
      "outputs": [],
      "source": [
        "#@title Create a URL and call the API for the DICOM volume\n",
        "\n",
        "TOKEN_ = !gcloud beta auth application-default print-access-token\n",
        "TOKEN = TOKEN_[0]\n",
        "\n",
        "my_url = create_lidc_series_url(study_uids[VOLUME_TO_SHOW],\n",
        "                                corresponding_series_uids[VOLUME_TO_SHOW])\n",
        "get_ct_embeddings(mytoken=TOKEN, dicom_url=my_url)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KTpU6d_WPXx8"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}